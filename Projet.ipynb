{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par mettre en place un algorithme SDCA classique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples, n_features = 1000, 5\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "w_real = np.random.randn(n_features)\n",
    "y = np.sign(X.dot(w_real) + np.random.randn(n_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Définition des fonctions de perte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit la fonction qui update alpha pour la hinge loss\n",
    "def update_hinge_loss(y, w, x, n, lamb, alpha, gamma=0):\n",
    "    \"\"\"\n",
    "        Update alpha pour la hinge loss\n",
    "        y: label\n",
    "        w: poids actuels\n",
    "        x: inputs\n",
    "        n: nombre de samples\n",
    "        lamb: step\n",
    "        alpha: variable du dual\n",
    "        gamma: smoothness\n",
    "    \"\"\"\n",
    "    minim = min(1, ((1-x.dot(w)*y)/(x.dot(x)/(lamb*n)))+alpha*y)\n",
    "    delta_alpha = y* max(0, minim)-alpha\n",
    "    return (delta_alpha)     \n",
    "    \n",
    "# On définit la fonction qui update alpha pour la absolute deviation loss \n",
    "def update_absolute_deviation_loss(y, w, x, n, lamb, alpha, gamma=0):\n",
    "    \"\"\"\n",
    "        Update alpha pour la absolute deviation loss\n",
    "        y: label\n",
    "        w: poids actuels\n",
    "        x: inputs\n",
    "        n: nombre de samples\n",
    "        lamb: step\n",
    "        alpha: variable du dual\n",
    "        gamma: smoothness\n",
    "    \"\"\"\n",
    "    minim = min(1, ((y-x.dot(w))/(x.dot(x)/(lamb*n)))+alpha)\n",
    "    delta_alpha = max(-1, minim)-alpha\n",
    "    return (delta_alpha)\n",
    "\n",
    "# On définit la fonction qui update alpha pour la smooth hinge loss \n",
    "def update_smoothed_hinge_loss(y, w, x, n, lamb, alpha, gamma):\n",
    "    \"\"\"\n",
    "        Update alpha pour la smoothed hinge loss \n",
    "        y: label\n",
    "        w: poids actuels\n",
    "        x: inputs\n",
    "        n: nombre de samples\n",
    "        lamb: step\n",
    "        alpha: variable du dual\n",
    "        gamma: smoothness\n",
    "    \"\"\"\n",
    "    minim = min(1, ((1-x.dot(w)*y-gamma*alpha*y)/(x.dot(x)/(lamb*n+gamma)))+alpha*y)\n",
    "    delta_alpha = y*max(0, minim)-alpha\n",
    "    return (delta_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de l'algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SDCA (X, y, T, update_loss, gamma, T_0_ratio, lamb = 1e-5 ):\n",
    "    \"\"\"\n",
    "        Algorithme SDCA classique\n",
    "        X: variables explicatives\n",
    "        y: label a prdire\n",
    "        T: nombre d'epoch\n",
    "        update_loss: choix de la loss function qui influe l'update de alpha\n",
    "        gamma: parametre de smoothing, 1 etant tres smooth\n",
    "        T_0_ratio: fraction qui indique quelle part des iterations on conserve\n",
    "                    pour moyenner et obtenir alpha et w finaux\n",
    "        lamb: step\n",
    "    \"\"\"\n",
    "   \n",
    "    n_samples = len(y)\n",
    "    n_features=X.shape[1]\n",
    "    T = T*n_samples\n",
    "    T_0 = int(T * T_0_ratio)\n",
    "    \n",
    "    #Initialisations\n",
    "    W=np.zeros(n_features*T_0).reshape(T_0,n_features)\n",
    "    Alpha = np.zeros(n_samples*T_0).reshape(T_0,n_samples)\n",
    "\n",
    "    #On fait tourner les premieres iterations sans sauvegarder les resultats\n",
    "\n",
    "    for t in range (0,T-T_0):\n",
    "        i = np.random.random_integers(0,n_samples-1)\n",
    "        delta_alpha = update_loss(y[i], W[0], X[i], n_samples, lamb, Alpha[0][i], gamma)\n",
    "\n",
    "        # On update alpha\n",
    "        Alpha[0] = Alpha[0] \n",
    "        # On update la coordonnée de alpha qui correspond à l'observation choisie\n",
    "        Alpha[0][i]+=delta_alpha\n",
    "\n",
    "        # On update w\n",
    "        W[0] = W[0]+ (X[i].dot(delta_alpha))\n",
    "\n",
    "    #Puis on sauvegarde le reste\n",
    "\n",
    "    for t in range (0,T_0-1):\n",
    "        i = np.random.random_integers(0,n_samples-1)\n",
    "        delta_alpha = update_loss(y[i], W[t], X[i], n_samples, lamb, Alpha[t][i], gamma)\n",
    "\n",
    "        # On update alpha\n",
    "        Alpha[t+1] = Alpha[t] \n",
    "        # On update la coordonnée de alpha qui correspond à l'observation choisie\n",
    "        Alpha[t+1][i]+=delta_alpha\n",
    "\n",
    "        # On update w\n",
    "        W[t+1] = W[t]+ (X[i].dot(delta_alpha))\n",
    "\n",
    "    # Sortie en moyenne\n",
    "    alpha_bar = Alpha.mean(axis=0)\n",
    "    w_bar = W.mean(axis=0)\n",
    "    print(\"true weights\", w_real)\n",
    "    print(\"-------------------------------------\")\n",
    "    print(\"w bar\",w_bar)\n",
    "    print(\"squared norm of difference\",(w_real-w_bar).dot(w_real-w_bar))\n",
    "    \n",
    "    # Sortie aleatoire\n",
    "    j = np.random.random_integers(0,T_0-1)\n",
    "    alpha_rand = Alpha[j]\n",
    "    w_rand=W[j]\n",
    "    print(\"-------------------------------------\")\n",
    "    print(\"w random\",w_rand)\n",
    "    print(\"squared norm of difference\",(w_real-w_rand).dot(w_real-w_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On fait tourner l'algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:30: DeprecationWarning: This function is deprecated. Please call randint(0, 999 + 1) instead\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:44: DeprecationWarning: This function is deprecated. Please call randint(0, 999 + 1) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true weights [ 0.30972382 -0.73745619 -1.53691988 -0.56225483 -1.59951112]\n",
      "-------------------------------------\n",
      "w bar [ 0.19303797 -0.51860118 -1.16337421 -0.4275179  -1.25693034]\n",
      "squared norm of difference 0.336565111085642\n",
      "-------------------------------------\n",
      "w random [ 0.14624221 -0.58040864 -0.9776572  -0.55609604 -1.52462582]\n",
      "squared norm of difference 0.36981066425058257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:64: DeprecationWarning: This function is deprecated. Please call randint(0, 4999 + 1) instead\n"
     ]
    }
   ],
   "source": [
    "#Hyperparametres\n",
    "SDCA(X, y, 10, update_smoothed_hinge_loss, 1.0 , 1/2, 1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On introduit une Stochastic Gradient Descent pour la premiere epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On appelle $S$ un ensemble de points, $x_i \\in \\mathbb{R}^{n}$ et les labels correspondants, $y_i \\in \\{−1,1\\}$. On cherche un hyperplan qui minimiserait la hinge loss totale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "w^* = \\underset{w}{\\text{argmin }} L^{hinge}_S(w) = \\underset{w}{\\text{argmin }} \\sum_i{l_{hinge}(w,x_i,y_i)}= \\underset{w}{\\text{argmin }} \\sum_i{\\max{\\{0,1-y_iw\\cdot x}\\}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On identifie le gradient:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{l_{hinge}}}{\\partial w}=\n",
    "\\begin{cases}\n",
    "  0  & y_iw\\cdot x \\geq 1 \\\\\n",
    "  -y_ix & y_iw\\cdot x < 1\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(w,X,y):\n",
    "    \"\"\" \n",
    "    Evalue la hinge loss and son gradient en w\n",
    "    w: poids actuels\n",
    "    X: matrice des inputs\n",
    "    y: vecteur des labels\n",
    "    \"\"\"\n",
    "    loss,grad = 0,0\n",
    "    for (X_,y_) in zip(X,y):\n",
    "        v = y_*np.dot(w,X_)\n",
    "        loss += max(0,1-v)\n",
    "        grad += 0 if v >= 1 else -y_*x_\n",
    "    return (loss,grad)\n",
    "\n",
    "def grad_descent(X,y,w,step,thresh=0.0001):\n",
    "    grad = np.inf\n",
    "    ws = np.zeros((3,0))\n",
    "    ws = np.hstack((ws,w.reshape(3,1)))\n",
    "    step_num = 1\n",
    "    delta = np.inf\n",
    "    loss0 = np.inf\n",
    "    while np.abs(delta)>thresh:\n",
    "        loss,grad = hinge_loss(w,X,y)\n",
    "        delta = loss0-loss\n",
    "        loss0 = loss\n",
    "        grad_dir = grad/np.linalg.norm(grad)\n",
    "        w = w-step*grad_dir/step_num\n",
    "        ws = np.hstack((ws,w.reshape((3,1))))\n",
    "        step_num += 1\n",
    "    return np.sum(ws,1)/np.size(ws,1)\n",
    "\n",
    "def test1():\n",
    "    # sample data points\n",
    "    x1 = np.array((0,1,3,4,1))\n",
    "    x2 = np.array((1,2,0,1,1))\n",
    "    x3 = np.array((2,0,3,1,0))\n",
    "    x  = np.vstack((x1,x2,x3)).T\n",
    "    # sample labels\n",
    "    y = np.array((1,1,-1,-1,-1))\n",
    "    w = grad_descent(x,y,np.array((0,0,0)),0.1)\n",
    "    loss, grad = hinge_loss(w,x,y)\n",
    "    return(loss)\n",
    "    #plot_test(x,y,w)\n",
    "\n",
    "def plot_test(x,y,w):\n",
    "    plt.figure()\n",
    "    x1, x2 = x[:,0], x[:,1]\n",
    "    x1_min, x1_max = np.min(x1)*.7, np.max(x1)*1.3\n",
    "    x2_min, x2_max = np.min(x2)*.7, np.max(x2)*1.3\n",
    "    gridpoints = 2000\n",
    "    x1s = np.linspace(x1_min, x1_max, gridpoints)\n",
    "    x2s = np.linspace(x2_min, x2_max, gridpoints)\n",
    "    gridx1, gridx2 = np.meshgrid(x1s,x2s)\n",
    "    grid_pts = np.c_[gridx1.ravel(), gridx2.ravel()]\n",
    "    predictions = np.array([np.sign(np.dot(w,x_)) for x_ in grid_pts]).reshape((gridpoints,gridpoints))\n",
    "    plt.contourf(gridx1, gridx2, predictions, cmap=plt.cm.Paired)\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.title('total hinge loss: %g' % hinge_loss(w,x,y)[0])\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.set_printoptions(precision=3)\n",
    "    test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.447679373104917"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
